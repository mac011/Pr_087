{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial\n",
    "import utm\n",
    "import geopy as gp\n",
    "import geopy as gp\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10,10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Introduction:\n",
    "\n",
    "Our project was inspired by our own experiences with the problems of persistent potholes–a hinderance that seems to cause universal frustration. According to Wilkens, the process of fixing a pothole is prioritized by complaint tickets reported by the city, which indicates that pothole areas with the most complaints take precedence (Wilkens, 2017). In addition to this, there are merely \"eight two-person crews that work each 10 hours shift\" (Wilkens, 2017). This results in an inefficient method because the process of fixing potholes is not prioritized effectively to concentrate on the areas that need it most. \n",
    " \n",
    "Research Question:\n",
    "\n",
    "People have become so desensitized to potholes that their frustration for them have become an everyday routine. For the safety of people and their cars, we asked, what approaches can be taken towards increasing the efficiency of pothole repair? And where should San Diego be allocating their pothole workers and resources? \n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "We predict that the following areas of concern are where the largest problems of potholes reside. These include the hindering of the bus transit system, reducing revenue earned from nearby affected parking meters, and a correlation of a slower rate a pothole is fixed with neglected areas of lower socioeconomic status in San Diego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          ****need to figure out how to smoothly incorporate the Additional Data Analysis Ya'll did last night**** \n",
    "                           (time and date it took to fill in near bus stops and populations) \n",
    "                           *new bus stop data--> can go under bus transits*\n",
    "                           *new populations data--> can go under a completely new category*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         DATA SET: \"GET IT DONE\" (POTHOLES)\n",
    "\n",
    "First of all, \"get it done\" is a service that people report non-emergency problems to about a city such as San Diego. To begin our analyses of pothole impact, we needed to firstly clean the “Get it Done” data because it was filled with extraneous information about \"get it done\" reports such as graffiti locations, abandoned car locations, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/iCloud Drive/Desktop/COGS108/Pr_087/clean_data/cleaned_get_it_done_closed_potholes.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e72e49005fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#upload get it done data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/iCloud Drive/Desktop/COGS108/Pr_087/clean_data/cleaned_get_it_done_closed_potholes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dropping unnecessary data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sap_notification_number'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sap_problem_code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'service_subtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;34m'referred_email_update'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'referral_email'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'functional_location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;34m'agency_responsible'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'sap_problem_category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'case_record_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'updated_datetime'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'service_request_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parent_case_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maggie/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maggie/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maggie/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maggie/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maggie/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/iCloud Drive/Desktop/COGS108/Pr_087/clean_data/cleaned_get_it_done_closed_potholes.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#upload get it done data set\n",
    "df = pd.read_csv('/iCloud Drive/Desktop/COGS108/Pr_087/clean_data/cleaned_get_it_done_closed_potholes.csv')\n",
    "\n",
    "#dropping unnecessary data\n",
    "df = df.drop(['address', 'sap_notification_number', 'sap_problem_code', 'service_subtype', 'source', \\\n",
    "            'referred_email_update', 'referral_email', 'functional_location', 'description', \\\n",
    "            'agency_responsible' , 'sap_problem_category', 'case_record_type', 'updated_datetime',\\\n",
    "            'service_request_id', 'parent_case_number'], axis = 1)\n",
    "\n",
    "#drop all potholes \n",
    "df = df.drop(df[df.sap_problem_type != 'Pothole'].index)\n",
    "\n",
    "#drop all NaNs for Long and Lat\n",
    "df = df.dropna(subset = ['long'])\n",
    "df = df.dropna(subset = ['lat'])\n",
    "\n",
    "#drop all non-active potholes\n",
    "df = df.drop(df[df.status_description == 'Duplicate'].index)\n",
    "df = df.drop(df[df.status_description== 'Closed'].index)\n",
    "df = df.drop(df[df.status_description=='Closed - Referred'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data, we were left with the most relevant information about potholes: their latitude, longitude, and their status (open or closed). We then used this data with all the following data sets, as shown below in the rest of the report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           DATA SET: \"PARKING METERS LOCATIONS AND PARKING METERS TRANSACTIONS\"\n",
    "We looked into parking meter locations around San Diego, their vicinity to potholes, and whether that affected how much they were making. We began by cleaning the data and removing irrelevant columns for both the parking meters locations and parking meters transactions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#upload parking meters locations\n",
    "raw_trans = pd.read_csv('/raw_data/treas_meters_2017_pole_by_mo_day_datasd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#upload parking meters transactions \n",
    "clean_parking_loc = pd.read_csv('/clean_data/parking_meter_location.csv')\n",
    "\n",
    "#drop rows with zero longitude and latitudes \n",
    "clean_parking_loc = clean_parking_loc.drop(clean_parking_loc[clean_parking_loc.longitude==0.000000].index)\n",
    "\n",
    "#rename pole column header of clean_parking_loc data to 'pole_id' to match the header of the clean_combined_parking_meter_data dataset\n",
    "clean_parking_loc.columns= ['index', 'pole_id', 'longitude', 'latitude']\n",
    "\n",
    "#drop index column\n",
    "clean_parking_loc = clean_parking_loc.drop(['index'], axis = 1)\n",
    "\n",
    "#merge datasets based on matching parking meter IDs\n",
    "merged_id = pd.merge(clean_parking_loc, clean_trans, on='pole_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data set, we were left to work with the latitude, longitude, and pole identifications of the parking meters. The parking meter transactions were added up based on pole ID and then combined with the parking meter location data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sum of transaction amount based on pole_id\n",
    "raw_trans['combined_parking_meter_data'] = raw_trans.groupby(['pole_id'])['sum_trans_amt'].transform('sum')\n",
    "\n",
    "#drop columns 'meter_type', 'month', 'day', 'num_trans'\n",
    "clean_trans = raw_trans.drop(['meter_type', 'month', 'day', 'num_trans'],axis = 1)\n",
    "\n",
    "#eliminated duplicate sum value for each pole_id\n",
    "clean_trans = clean_trans.drop_duplicates(subset=['pole_id'], keep='first')\n",
    "\n",
    "#merge datasets based on matching parking meter IDs\n",
    "merged_id = pd.merge(clean_parking_loc, clean_trans, on='pole_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                        ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we compared the distribution of the distances of the parking meters to the closest pothole to random points around San Diego to the closest pothole. We were able to generate these random points by using the CA shapefile with QGIS, creating 60000 random points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#graphing (and code???) of both random points + min distance to parking meters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distances from Potholes to nearest Parking Meters\n",
    "open_pot = pd.read_csv('Pr_087/clean_data/cleaned_get_it_done_open_potholes.csv')\n",
    "open_pot = open_pot[~np.isnan(open_pot['long'])]\n",
    "open_pot = open_pot[open_pot['long'] != 0]\n",
    "\n",
    "meter_data = pd.read_csv('Pr_087/raw_data/raw_parking_meter_location.csv')\n",
    "meter_data = meter_data[~np.isnan(meter_data['longitude'])]\n",
    "meter_data = meter_data[meter_data['longitude'] != 0]\n",
    "\n",
    "open_pot_coordinates = np.asarray(open_pot[['lat','long']])\n",
    "meter_data_coordinates = np.asarray(meter_data[['latitude','longitude']])\n",
    "\n",
    "#remove outlier\n",
    "open_pot_coordinates = np.delete(open_pot_coordinates, 2293 ,axis=0) \n",
    "\n",
    "#Convert to utm\n",
    "open_pot_coordinates = latlon_to_utm(open_pot_coordinates)\n",
    "meter_data_coordinates = latlon_to_utm(meter_data_coordinates)\n",
    "\n",
    "dist_meter_data = scipy.spatial.distance.cdist(open_pot_coordinates,meter_data_coordinates)\n",
    "min_dist_indices = np.zeros(dist_meter_data.shape[0])\n",
    "min_dists = np.zeros(dist_meter_data.shape[0])\n",
    "for i in range(0,dist_meter_data.shape[0]):\n",
    "    _ = np.argmin(dist_meter_data[i])\n",
    "    min_dists[i] = dist_meter_data[i][_]\n",
    "    min_dist_indices[i] = _    \n",
    "    \n",
    "min_dists2 = np.zeros(dist_meter_data.shape[1])\n",
    "for j in range(0,dist_meter_data.shape[1]):\n",
    "    _ = np.argmin(dist_meter_data.T[j])\n",
    "    min_dists2[j] = dist_meter_data[_][j]\n",
    "#merge meters with min dist to pothole\n",
    "meter_data['nearest_pothole_dist'] = min_dists2\n",
    "meter_data.to_csv('TimLeeBro.csv')\n",
    "    \n",
    "plt.close()\n",
    "plt.figure(figsize=(10,10))\n",
    "# plt.hist(np.log(min_dists), 50)\n",
    "plt.hist(min_dists, 50)\n",
    "plt.title('Distances from Potholes to nearest Parking Meters')\n",
    "# plt.xlabel('Distance in meters (logged)')\n",
    "plt.xlabel('Distance in meters')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Mean:',np.mean(min_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distances from Random Points to Parking Meters\n",
    "random_points = pd.read_csv('Pr_087/clean_data/randPoints6000.csv')\n",
    "random_points_coordinates = np.asarray(random_points[['Y','X']])\n",
    "\n",
    "#convert to utm\n",
    "random_points_coordinates = latlon_to_utm(random_points_coordinates)\n",
    "\n",
    "dist_meter_data = scipy.spatial.distance.cdist(random_points_coordinates,meter_data_coordinates)\n",
    "min_dist_indices = np.zeros(dist_meter_data.shape[0])\n",
    "min_dists = np.zeros(dist_meter_data.shape[0])\n",
    "for i in range(0,dist_meter_data.shape[0]):\n",
    "    _ = np.argmin(dist_meter_data[i])\n",
    "    min_dists[i] = dist_meter_data[i][_]\n",
    "    min_dist_indices[i] = _    \n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(10,10))\n",
    "# plt.hist(np.log(min_dists), 50)\n",
    "plt.hist(min_dists, 50)\n",
    "plt.title('Distances from Random Points to Parking Meters')\n",
    "# plt.xlabel('Distance in meters (Logged)')\n",
    "plt.xlabel('Distance in meters')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Mean:',np.mean(min_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that these graphs did not show any evidence of significant data, especially because there is evidently a large concentration of parking in meters in one area, as seen in the graph above. Additionally, the average results of each of these distributions do not have a major difference between them, which is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert averages of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                        ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we did not need further analysis, we wanted to visualize and see where the parking meters were located using Arc GIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TIM'S GIS GRAPH-- probs .load \n",
    "# arcgis = Image.open(\"your_image_here\");\n",
    "# arcgis.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of this visualization, we concluded that it was not wise to further analyze this data because most of the parking meters listed from this set is concentrated in specific areas mainly in the downtown region, as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         DATA SET: \"TRANSIT STOPS LOCATIONS\" \n",
    "\n",
    "We followed a similar procedure to that of the parking meters for analyzing the bus stop locations. We started by cleaning the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #upload bus stops\n",
    "# bus_stops= pd.read_csv('/raw_data/CITY.TRANSIT_STOPS_GTFS_datasd.csv')\n",
    "\n",
    "# #dropping irrelevant columns \n",
    "# clean_bus_stops = bus_stops[['LONGITUDE', 'LATITUDE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were left with just the latitude and longitude of the bus stops, which we would again then compare the distances to the potholes with the random points obtained from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert graphs (and code???) of random points + min distance to bus stops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distances of Potholes to nearest Bus Stop\n",
    "dist_bus_data = scipy.spatial.distance.cdist(open_pot_coordinates,bus_stops_coordinates)\n",
    "min_dist_indices = np.zeros(dist_bus_data.shape[0])\n",
    "min_dists = np.zeros(dist_bus_data.shape[0])\n",
    "\n",
    "for i in range(0,dist_bus_data.shape[0]):\n",
    "    _ = np.argmin(dist_bus_data[i])\n",
    "    min_dists[i] = dist_bus_data[i][_]\n",
    "    min_dist_indices[i] = _   \n",
    "    #Filter outliers\n",
    "#     if dist_bus_data[i][_] > 7160:\n",
    "#         print('WTF',i,':',dist_bus_data[i][_])\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(10,10))\n",
    "# plt.hist(np.log(min_dists), 50)\n",
    "plt.hist(min_dists,50)\n",
    "plt.title('Distances of Potholes to nearest Bus Stop')\n",
    "# plt.xlabel('Distance in meters (logged)')\n",
    "plt.xlabel('Distance in meters')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Mean:',np.mean(min_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Distances from Random Locations to nearest Bus Stops\n",
    "plt.close()\n",
    "plt.close()\n",
    "random_points = pd.read_csv('Pr_087/clean_data/randomPoints.csv')\n",
    "random_points_coordinates = np.asarray(random_points[['Y','X']])\n",
    "random_points_coordinates = latlon_to_utm(random_points_coordinates)\n",
    "dist_bus_data = scipy.spatial.distance.cdist(random_points_coordinates,bus_stops_coordinates)\n",
    "print(np.mean(dist_bus_data))\n",
    "min_dist_indices = np.zeros(bus_data.shape[0])\n",
    "min_dists = np.zeros(dist_bus_data.shape[0])\n",
    "for i in range(0,bus_data.shape[0]):\n",
    "    _ = np.argmin(dist_bus_data[i])\n",
    "    min_dists[i] = dist_bus_data[i][_]\n",
    "    min_dist_indices[i] = _    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(np.log(min_dists), 50)\n",
    "# plt.hist(min_dists, 50)\n",
    "plt.title('Distances from Random Locations to nearest Bus Stops')\n",
    "plt.xlabel('Distances in meters logged')\n",
    "# plt.xlabel('Distances in meters')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Mean:',np.mean(min_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From both distribution graphs, we were able to conclude that there was actually a significant result. As seen by the averages below, the averages of the minimum distance of potholes to the bus stops compared with the random points is much smaller. This indicates that the results are not just by chance; there are more potholes allocated near bus stop locations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                DATA SET: \"Neighborhood_MedianValuePerSqft_AllHomes\" \n",
    "\n",
    "We then used Zillow’s price per square foot data to determine the socioeconomic effect of potholes. We first cleaned data to only use the locations based in San Diego. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #upload zillow data\n",
    "# zillowpd = pd.read_csv('/raw_data/Neighborhood_MedianValuePerSqft_AllHomes.csv') \n",
    "\n",
    "# #remove all counties not san diego\n",
    "# zillowpdsd = zillowpd[zillowpd['CountyName'] == 'San Diego'] \n",
    "# #remove uneccessary columns\n",
    "# zillowpdsd = zillowpdsd.drop('RegionID', 1)\n",
    "# zillowpdsd = zillowpdsd.drop('State', 1)\n",
    "# zillowpdsd = zillowpdsd.drop('SizeRank', 1)\n",
    "# #remove all but latest rent info\n",
    "# zillowpdsd2 = zillowpdsd[['RegionName', 'City','Metro','CountyName','2017-03']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the Zillow data only contained neighborhoods, thus we were not able to directly map the price/sqft with the potholes. So, we used geocoder with the pothole data to determine the neighborhoods of each pothole, which we could then map to the Zillow prices. Yet, we found that some names did not match, which we found that we needed to use Bing API, which Zillow uses for its neighborhoods to fill in the missing gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert some google/ bing api?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the actual neighborhoods where all the potholes resided, we compared this data with the time it took for a pothole to close and measured the correlation between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tim cleaning up to get time \n",
    "#tim graphs on time + neighborhoods and also Pearson/Spearman stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the data is normally distributed. There is no real correlation between the time it takes for a pothole to close and the socio-economic status of a neighborhood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Conclusion/ Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*need general results but does not need to be fleshed out for tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John Wilkens 2017: http://www.sandiegouniontribune.com/news/transportation/sd-me-pothole-repair-20170208-story.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
